{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7749004a",
   "metadata": {},
   "source": [
    "# Visualizing convnet filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6640eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuhaochen/Developer/LearnDeepLearning/InterpretConvnet/.pixi/envs/default/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Xception Preprocessor Setup ===\n",
      "\n",
      "Attempting to resolve Xception preprocessor...\n",
      "\n",
      "✓ Successfully patched ImageConverter to handle antialias parameter\n",
      "✓ Successfully loaded preprocessor from preset!\n",
      "\n",
      "✓ Success using method: preset_with_patch\n",
      "Preprocessor type: <class 'keras_hub.src.layers.preprocessing.image_converter.ImageConverter'>\n",
      "✗ All solutions failed: Exception encountered when calling ImageConverter.call().\n",
      "\n",
      "\u001b[1mcannot compute Mul as input #1(zero-based) was expected to be a uint8 tensor but is a float tensor [Op:Mul] name: \u001b[0m\n",
      "\n",
      "Arguments received by ImageConverter.call():\n",
      "  • inputs=array([[[[149, 218,  66],\n",
      "         [165,   0, 151],\n",
      "         [ 12, 184, 232],\n",
      "         ...,\n",
      "         [ 19, 161, 226],\n",
      "         [ 30, 163, 191],\n",
      "         [ 29,   3, 132]],\n",
      "\n",
      "        [[ 87,  37, 110],\n",
      "         [162,   7, 134],\n",
      "         [ 32, 112, 159],\n",
      "         ...,\n",
      "         [242, 178, 227],\n",
      "         [ 75, 105,  23],\n",
      "         [162, 108,  95]],\n",
      "\n",
      "        [[225, 253,  14],\n",
      "         [ 50, 152, 163],\n",
      "         [ 83,  79, 197],\n",
      "         ...,\n",
      "         [ 97, 218, 159],\n",
      "         [242, 102,  86],\n",
      "         [179, 191, 194]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[162,  15, 175],\n",
      "         [221, 224, 210],\n",
      "         [  1, 122, 101],\n",
      "         ...,\n",
      "         [118, 132,  54],\n",
      "         [218, 123, 122],\n",
      "         [151,  65, 215]],\n",
      "\n",
      "        [[  6, 244, 197],\n",
      "         [137, 113, 241],\n",
      "         [ 21,  70, 175],\n",
      "         ...,\n",
      "         [ 10,  39, 102],\n",
      "         [251,  99, 253],\n",
      "         [ 31, 185, 150]],\n",
      "\n",
      "        [[  7,  42, 138],\n",
      "         [254,  20,   2],\n",
      "         [ 35,  15, 190],\n",
      "         ...,\n",
      "         [150, 101,  41],\n",
      "         [121, 178,  83],\n",
      "         [ 38,  18, 109]]]], dtype=uint8)\n",
      "\n",
      "Try updating your packages:\n",
      "pip install --upgrade keras-hub tensorflow\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from keras import layers\n",
    "from keras_hub.src.api_export import keras_hub_export\n",
    "from keras_hub.src.models.backbone import Backbone\n",
    "from keras_hub.src.layers.preprocessing.image_converter import ImageConverter\n",
    "from keras_hub.src.utils.keras_utils import standardize_data_format\n",
    "\n",
    "\n",
    "@keras_hub_export(\"keras_hub.models.XceptionBackbone\")\n",
    "class XceptionBackbone(Backbone):\n",
    "    \"\"\"Xception core network with hyperparameters.\n",
    "\n",
    "    This class implements a Xception backbone as described in\n",
    "    [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357).\n",
    "\n",
    "    Most users will want the pretrained presets available with this model. If\n",
    "    you are creating a custom backbone, this model provides customizability\n",
    "    through the `stackwise_conv_filters` and `stackwise_pooling` arguments. This\n",
    "    backbone assumes the same basic structure as the original Xception mode:\n",
    "    * Residuals and pre-activation everywhere but the first and last block.\n",
    "    * Conv layers for the first block only, separable conv layers elsewhere.\n",
    "\n",
    "    Args:\n",
    "        stackwise_conv_filters: list of list of ints. Each outermost list\n",
    "            entry represents a block, and each innermost list entry a conv\n",
    "            layer. The integer value specifies the number of filters for the\n",
    "            conv layer.\n",
    "        stackwise_pooling: list of bools. A list of booleans per block, where\n",
    "            each entry is true if the block should includes a max pooling layer\n",
    "            and false if it should not.\n",
    "        image_shape: tuple. The input shape without the batch size.\n",
    "            Defaults to `(None, None, 3)`.\n",
    "        data_format: `None` or str. If specified, either `\"channels_last\"` or\n",
    "            `\"channels_first\"`. If unspecified, the Keras default will be used.\n",
    "        dtype: `None` or str or `keras.mixed_precision.DTypePolicy`. The dtype\n",
    "            to use for the model's computations and weights.\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    input_data = np.random.uniform(0, 1, size=(2, 224, 224, 3))\n",
    "\n",
    "    # Pretrained Xception backbone.\n",
    "    model = keras_hub.models.Backbone.from_preset(\"xception_41_imagenet\")\n",
    "    model(input_data)\n",
    "\n",
    "    # Randomly initialized Xception backbone with a custom config.\n",
    "    model = keras_hub.models.XceptionBackbone(\n",
    "        stackwise_conv_filters=[[32, 64], [64, 128], [256, 256]],\n",
    "        stackwise_pooling=[True, True, False],\n",
    "    )\n",
    "    model(input_data)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        stackwise_conv_filters,\n",
    "        stackwise_pooling,\n",
    "        image_shape=(None, None, 3),\n",
    "        data_format=None,\n",
    "        dtype=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if len(stackwise_conv_filters) != len(stackwise_pooling):\n",
    "            raise ValueError(\"All stackwise args should have the same length.\")\n",
    "\n",
    "        data_format = standardize_data_format(data_format)\n",
    "        channel_axis = -1 if data_format == \"channels_last\" else 1\n",
    "        num_blocks = len(stackwise_conv_filters)\n",
    "\n",
    "        # Layer shorcuts with common args.\n",
    "        norm = functools.partial(\n",
    "            layers.BatchNormalization,\n",
    "            axis=channel_axis,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        act = functools.partial(\n",
    "            layers.Activation,\n",
    "            activation=\"relu\",\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        conv = functools.partial(\n",
    "            layers.Conv2D,\n",
    "            kernel_size=(3, 3),\n",
    "            use_bias=False,\n",
    "            data_format=data_format,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        sep_conv = functools.partial(\n",
    "            layers.SeparableConv2D,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "            data_format=data_format,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        point_conv = functools.partial(\n",
    "            layers.Conv2D,\n",
    "            kernel_size=(1, 1),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "            data_format=data_format,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        pool = functools.partial(\n",
    "            layers.MaxPool2D,\n",
    "            pool_size=(3, 3),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            data_format=data_format,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "        # === Functional Model ===\n",
    "        image_input = layers.Input(shape=image_shape)\n",
    "        x = image_input  # Intermediate result.\n",
    "\n",
    "        # Iterate through the blocks.\n",
    "        for block_i in range(num_blocks):\n",
    "            first_block, last_block = block_i == 0, block_i == num_blocks - 1\n",
    "            block_filters = stackwise_conv_filters[block_i]\n",
    "            use_pooling = stackwise_pooling[block_i]\n",
    "\n",
    "            # Save the block input as a residual.\n",
    "            residual = x\n",
    "            for conv_i, filters in enumerate(block_filters):\n",
    "                # First block has post activation and strides on first conv.\n",
    "                if first_block:\n",
    "                    prefix = f\"block{block_i + 1}_conv{conv_i + 1}\"\n",
    "                    strides = (2, 2) if conv_i == 0 else (1, 1)\n",
    "                    x = conv(filters, strides=strides, name=prefix)(x)\n",
    "                    x = norm(name=f\"{prefix}_bn\")(x)\n",
    "                    x = act(name=f\"{prefix}_act\")(x)\n",
    "                # Last block has post activation.\n",
    "                elif last_block:\n",
    "                    prefix = f\"block{block_i + 1}_sepconv{conv_i + 1}\"\n",
    "                    x = sep_conv(filters, name=prefix)(x)\n",
    "                    x = norm(name=f\"{prefix}_bn\")(x)\n",
    "                    x = act(name=f\"{prefix}_act\")(x)\n",
    "                else:\n",
    "                    prefix = f\"block{block_i + 1}_sepconv{conv_i + 1}\"\n",
    "                    # The first conv in second block has no activation.\n",
    "                    if block_i != 1 or conv_i != 0:\n",
    "                        x = act(name=f\"{prefix}_act\")(x)\n",
    "                    x = sep_conv(filters, name=prefix)(x)\n",
    "                    x = norm(name=f\"{prefix}_bn\")(x)\n",
    "\n",
    "            # Optional block pooling.\n",
    "            if use_pooling:\n",
    "                x = pool(name=f\"block{block_i + 1}_pool\")(x)\n",
    "\n",
    "            # Sum residual, first and last block do not have a residual.\n",
    "            if not first_block and not last_block:\n",
    "                prefix = f\"block{block_i + 1}_residual\"\n",
    "                filters = x.shape[channel_axis]\n",
    "                # Match filters with a pointwise conv if needed.\n",
    "                if filters != residual.shape[channel_axis]:\n",
    "                    residual = point_conv(filters, name=f\"{prefix}_conv\")(\n",
    "                        residual\n",
    "                    )\n",
    "                    residual = norm(name=f\"{prefix}_bn\")(residual)\n",
    "                x = layers.Add(name=f\"{prefix}_add\", dtype=dtype)([x, residual])\n",
    "\n",
    "        super().__init__(\n",
    "            inputs=image_input,\n",
    "            outputs=x,\n",
    "            dtype=dtype,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # === Config ===\n",
    "        self.stackwise_conv_filters = stackwise_conv_filters\n",
    "        self.stackwise_pooling = stackwise_pooling\n",
    "        self.image_shape = image_shape\n",
    "        self.data_format = data_format\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"stackwise_conv_filters\": self.stackwise_conv_filters,\n",
    "                \"stackwise_pooling\": self.stackwise_pooling,\n",
    "                \"image_shape\": self.image_shape,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "@keras_hub_export(\"keras_hub.layers.XceptionImageConverter\")\n",
    "class XceptionImageConverter(ImageConverter):\n",
    "    \"\"\"Image converter for Xception models that handles legacy parameters.\"\"\"\n",
    "    \n",
    "    backbone_cls = XceptionBackbone\n",
    "    \n",
    "    def __init__(self, antialias=None, **kwargs):\n",
    "        \"\"\"Initialize XceptionImageConverter.\n",
    "        \n",
    "        Args:\n",
    "            antialias: Legacy parameter that is ignored for compatibility.\n",
    "            **kwargs: Arguments passed to the parent ImageConverter.\n",
    "        \"\"\"\n",
    "        # Remove antialias from kwargs if it exists (for compatibility)\n",
    "        kwargs.pop('antialias', None)\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Create layer from config, handling legacy antialias parameter.\"\"\"\n",
    "        # Remove antialias from config if it exists\n",
    "        config = config.copy()\n",
    "        config.pop('antialias', None)\n",
    "        return super().from_config(config)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Get configuration dict, excluding antialias for compatibility.\"\"\"\n",
    "        config = super().get_config()\n",
    "        # Remove antialias if it exists in the config\n",
    "        config.pop('antialias', None)\n",
    "        return config\n",
    "\n",
    "\n",
    "# SOLUTION 1: Direct base class patching (most reliable)\n",
    "def patch_image_converter_for_antialias():\n",
    "    \"\"\"Permanently patch ImageConverter to handle antialias parameter.\"\"\"\n",
    "    \n",
    "    # Store original methods\n",
    "    if not hasattr(ImageConverter, '_original_init'):\n",
    "        ImageConverter._original_init = ImageConverter.__init__\n",
    "        ImageConverter._original_from_config = ImageConverter.from_config\n",
    "    \n",
    "    def patched_init(self, antialias=None, **kwargs):\n",
    "        \"\"\"Patched init that ignores antialias parameter.\"\"\"\n",
    "        kwargs.pop('antialias', None)  # Remove antialias if present\n",
    "        ImageConverter._original_init(self, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def patched_from_config(cls, config):\n",
    "        \"\"\"Patched from_config that ignores antialias parameter.\"\"\"\n",
    "        config = config.copy()\n",
    "        config.pop('antialias', None)  # Remove antialias if present\n",
    "        return ImageConverter._original_from_config(config)\n",
    "    \n",
    "    # Apply patches\n",
    "    ImageConverter.__init__ = patched_init\n",
    "    ImageConverter.from_config = patched_from_config\n",
    "    print(\"✓ Successfully patched ImageConverter to handle antialias parameter\")\n",
    "\n",
    "\n",
    "def create_xception_preprocessor_with_preset():\n",
    "    \"\"\"Create Xception preprocessor from preset after patching.\"\"\"\n",
    "    import keras_hub\n",
    "    \n",
    "    # Apply the patch first\n",
    "    patch_image_converter_for_antialias()\n",
    "    \n",
    "    # Now load the preset\n",
    "    preprocessor = keras_hub.layers.ImageConverter.from_preset(\n",
    "        \"hf://keras/xception_41_imagenet\",\n",
    "        image_size=(180, 180),\n",
    "    )\n",
    "    print(\"✓ Successfully loaded preprocessor from preset!\")\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "# SOLUTION 2: Manual creation with exact preset settings\n",
    "def create_xception_preprocessor_manual():\n",
    "    \"\"\"Create Xception preprocessor manually with exact preset settings.\"\"\"\n",
    "    # These are the exact values from the preset config in the error message\n",
    "    preprocessor = ImageConverter(\n",
    "        image_size=(180, 180),\n",
    "        scale=0.00784313725490196,  # Exact value from preset\n",
    "        offset=-1.0,\n",
    "        interpolation=\"bilinear\",\n",
    "        crop_to_aspect_ratio=True,\n",
    "        pad_to_aspect_ratio=False,\n",
    "        bounding_box_format=\"yxyx\",\n",
    "        name=\"image_converter\"\n",
    "    )\n",
    "    print(\"✓ Created ImageConverter manually with exact preset settings!\")\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "# SOLUTION 3: Simple Keras preprocessing pipeline\n",
    "def create_simple_xception_preprocessor():\n",
    "    \"\"\"Create a simple preprocessing pipeline for Xception.\"\"\"\n",
    "    import keras\n",
    "    \n",
    "    preprocessor = keras.Sequential([\n",
    "        keras.layers.Resizing(180, 180, interpolation=\"bilinear\", crop_to_aspect_ratio=True),\n",
    "        keras.layers.Rescaling(scale=0.00784313725490196, offset=-1.0)  # Exact preset values\n",
    "    ], name=\"xception_preprocessor\")\n",
    "    \n",
    "    print(\"✓ Created simple Keras preprocessing pipeline!\")\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "# SOLUTION 4: Complete Xception model loading workaround\n",
    "def load_complete_xception_model():\n",
    "    \"\"\"Load the complete Xception model and extract preprocessor settings.\"\"\"\n",
    "    import keras_hub\n",
    "    \n",
    "    try:\n",
    "        # Try to load just the backbone first\n",
    "        backbone = keras_hub.models.XceptionBackbone.from_preset(\"hf://keras/xception_41_imagenet\")\n",
    "        print(\"✓ Successfully loaded Xception backbone!\")\n",
    "        \n",
    "        # Create compatible preprocessor\n",
    "        preprocessor = create_simple_xception_preprocessor()\n",
    "        \n",
    "        return backbone, preprocessor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Could not load backbone: {e}\")\n",
    "        return None, create_simple_xception_preprocessor()\n",
    "\n",
    "\n",
    "# Auto-select best solution\n",
    "def get_xception_preprocessor():\n",
    "    \"\"\"Automatically get a working Xception preprocessor.\"\"\"\n",
    "    \n",
    "    print(\"Attempting to resolve Xception preprocessor...\\n\")\n",
    "    \n",
    "    # Try Solution 1: Preset with patch\n",
    "    try:\n",
    "        preprocessor = create_xception_preprocessor_with_preset()\n",
    "        return preprocessor, \"preset_with_patch\"\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Preset loading failed: {str(e)[:100]}...\\n\")\n",
    "    \n",
    "    # Try Solution 2: Manual creation\n",
    "    try:\n",
    "        preprocessor = create_xception_preprocessor_manual()\n",
    "        return preprocessor, \"manual_imageconverter\"\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Manual ImageConverter failed: {str(e)[:100]}...\\n\")\n",
    "    \n",
    "    # Fallback to Solution 3: Simple pipeline\n",
    "    try:\n",
    "        preprocessor = create_simple_xception_preprocessor()\n",
    "        return preprocessor, \"simple_pipeline\"\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Simple pipeline failed: {str(e)[:100]}...\\n\")\n",
    "        raise Exception(\"All preprocessing solutions failed!\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "print(\"=== Xception Preprocessor Setup ===\\n\")\n",
    "\n",
    "try:\n",
    "    preprocessor, method = get_xception_preprocessor()\n",
    "    print(f\"\\n✓ Success using method: {method}\")\n",
    "    print(f\"Preprocessor type: {type(preprocessor)}\")\n",
    "    \n",
    "    # Test the preprocessor\n",
    "    import numpy as np\n",
    "    test_image = np.random.randint(0, 255, (1, 224, 224, 3), dtype=np.uint8)\n",
    "    \n",
    "    processed = preprocessor(test_image)\n",
    "    print(f\"\\n✓ Preprocessing test successful!\")\n",
    "    print(f\"  Input shape: {test_image.shape}\")\n",
    "    print(f\"  Output shape: {processed.shape}\")\n",
    "    print(f\"  Output range: [{np.min(processed):.4f}, {np.max(processed):.4f}]\")\n",
    "    print(f\"  Expected range for Xception: [-1.0, 1.0]\")\n",
    "    \n",
    "    # Check if output is in correct range for Xception\n",
    "    if -1.1 <= np.min(processed) <= -0.9 and 0.9 <= np.max(processed) <= 1.1:\n",
    "        print(\"  ✓ Output range looks correct for Xception!\")\n",
    "    else:\n",
    "        print(\"  ⚠ Output range may not be optimal for Xception\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ All solutions failed: {e}\")\n",
    "    print(\"\\nTry updating your packages:\")\n",
    "    print(\"pip install --upgrade keras-hub tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a442960a",
   "metadata": {},
   "source": [
    "Could not follow the tutorial, it seems like xception_41_imagenet and it's backbone no longer exista in newer version of Keras. I ended up copying the definition of XceptionBackbone straight from source: https://github.com/keras-team/keras-hub/blob/v0.21.1/keras_hub/src/models/xception/xception_backbone.py#L10\n",
    "and model straight from hugging_face: https://huggingface.co/keras/xception_41_imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3d8a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XceptionBackbone.from_preset(\"hf://keras/xception_41_imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e151f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = XceptionImageConverter.from_preset(\n",
    "    \"hf://keras/xception_41_imagenet\",\n",
    "    image_size=(180, 180),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6483773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1\n",
      "block1_conv2\n",
      "block2_sepconv1\n",
      "block2_sepconv2\n",
      "block2_residual_conv\n",
      "block3_sepconv1\n",
      "block3_sepconv2\n",
      "block3_residual_conv\n",
      "block4_sepconv1\n",
      "block4_sepconv2\n",
      "block4_residual_conv\n",
      "block5_sepconv1\n",
      "block5_sepconv2\n",
      "block5_sepconv3\n",
      "block6_sepconv1\n",
      "block6_sepconv2\n",
      "block6_sepconv3\n",
      "block7_sepconv1\n",
      "block7_sepconv2\n",
      "block7_sepconv3\n",
      "block8_sepconv1\n",
      "block8_sepconv2\n",
      "block8_sepconv3\n",
      "block9_sepconv1\n",
      "block9_sepconv2\n",
      "block9_sepconv3\n",
      "block10_sepconv1\n",
      "block10_sepconv2\n",
      "block10_sepconv3\n",
      "block11_sepconv1\n",
      "block11_sepconv2\n",
      "block11_sepconv3\n",
      "block12_sepconv1\n",
      "block12_sepconv2\n",
      "block12_sepconv3\n",
      "block13_sepconv1\n",
      "block13_sepconv2\n",
      "block13_residual_conv\n",
      "block14_sepconv1\n",
      "block14_sepconv2\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):\n",
    "        print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b34f5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = \"block3_sepconv1\"\n",
    "layer = model.get_layer(name=layer_name)\n",
    "feature_extractor = keras.Model(inputs=model.input, outputs=layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0325b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = keras.utils.get_file(\n",
    "    fname=\"cat.jpg\", origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\"\n",
    ")\n",
    "\n",
    "def get_img_array(img_path, target_size):\n",
    "    img = keras.utils.load_img(img_path, target_size=target_size)\n",
    "    array = keras.utils.img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "img_tensor = get_img_array(img_path, target_size=(180, 180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076878a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = feature_extractor(preprocessor(img_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dcf6801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import ops\n",
    "\n",
    "def compute_loss(image, filter_index):\n",
    "    activation = feature_extractor(image)\n",
    "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
    "    return ops.mean(filter_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e9377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
